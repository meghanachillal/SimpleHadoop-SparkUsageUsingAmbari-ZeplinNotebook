{"paragraphs":[{"text":"import org.apache.spark.sql.SparkSession\r\n\r\nval df = spark.read.json(\"/user/guest/flume/18-07-08/Log.1531080665465\")\r\n\r\nval df2 = spark.read.csv(\"/user/guest/flume/18-07-08/Log.1531015974737\")\r\n\r\nval df3 = spark.read.csv(\"/user/guest/flume/18-07-08/Log.1531015921443\")\r\n\r\n// Displays the content of the DataFrame to stdout\r\ndf.show()\r\n\r\ndf2.show()\r\n\r\ndf3.show()\r\n\r\n","user":"anonymous","dateUpdated":"2018-07-08T21:06:27+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala"},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"import org.apache.spark.sql.SparkSession\ndf: org.apache.spark.sql.DataFrame = [cities: array<string>, name: string ... 1 more field]\ndf2: org.apache.spark.sql.DataFrame = [_c0: string]\ndf3: org.apache.spark.sql.DataFrame = [_c0: string]\n+--------------------+-------+--------------------+\n|              cities|   name|             schools|\n+--------------------+-------+--------------------+\n|[palo alto, menlo...|Michael|[[stanford, 2010]...|\n|        [santa cruz]|   Andy|      [[ucsb, 2011]]|\n|          [portland]| Justin|  [[berkeley, 2014]]|\n+--------------------+-------+--------------------+\n\n+--------------------+\n|                 _c0|\n+--------------------+\n|This is the secon...|\n|This is the secon...|\n+--------------------+\n\n+--------------------+\n|                 _c0|\n+--------------------+\n|                   {|\n|  \"squadName\": \"S...|\n|  \"homeTown\": \"Me...|\n|      \"formed\": 2016|\n|  \"secretBase\": \"...|\n|      \"active\": true|\n|                   }|\n|Copy the CSV File...|\n|                   `|\n|This is some data...|\n+--------------------+\n\n"}]},"apps":[],"jobName":"paragraph_1531076294603_-1063004974","id":"20180708-185814_1768528750","dateCreated":"2018-07-08T18:58:14+0000","dateStarted":"2018-07-08T20:21:17+0000","dateFinished":"2018-07-08T20:21:41+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:233"},{"text":"import org.apache.spark.sql.types._\r\n\r\n// Create an RDD\r\nval myRDD = spark.sparkContext.textFile(\"/user/guest/flume/18-07-08/Log.1531015921443\")\r\n\r\n\r\nmyRDD.collect().foreach(println)\r\n\r\nmyRDD.count()","user":"anonymous","dateUpdated":"2018-07-08T20:32:12+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala"},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"import org.apache.spark.sql.types._\nmyRDD: org.apache.spark.rdd.RDD[String] = /user/guest/flume/18-07-08/Log.1531015921443 MapPartitionsRDD[236] at textFile at <console>:87\n{\n  \"squadName\": \"Super hero squad\",\n  \"homeTown\": \"Metro City\",\n  \"formed\": 2016,\n  \"secretBase\": \"Super tower\",\n  \"active\": true\n}\nCopy the CSV File to the Ambari !! FLUME\n`\nThis is some data to upload to hadoop. First test on Hadoop and Ambari....Success!!!!\nres138: Long = 10\n"}]},"apps":[],"jobName":"paragraph_1531076514781_1243670307","id":"20180708-190154_1285822390","dateCreated":"2018-07-08T19:01:54+0000","dateStarted":"2018-07-08T20:32:13+0000","dateFinished":"2018-07-08T20:32:17+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:234"},{"text":"import org.apache.spark.sql.types._\r\n\r\nimport org.apache.spark.sql.SparkSession\r\n\r\nval df = spark.read.json(\"/user/guest/flume/18-07-08/Log.1531080665465\")\r\n\r\n//Filter for DataFrame\r\nval report = df.select (\"*\").where (array_contains (df(\"cities\"), \"portland\"))\r\n\r\nreport.show()\r\n\r\n// Create an RDD\r\n\r\nval rdd1 = spark.sparkContext.textFile(\"/user/guest/flume/18-07-08/Log.1531015974737\")\r\n\r\nrdd1.collect().foreach(println)\r\n\r\n//Filter the RDD\r\n\r\nval newRDD = rdd1.filter(line => line.contains(\"a\")).count()\r\n\r\nprintln(newRDD)\r\n\r\nprintln(s\"Lines with a: $newRDD\")\r\n \r\n ","user":"anonymous","dateUpdated":"2018-07-08T21:47:43+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala"},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"import org.apache.spark.sql.types._\nimport org.apache.spark.sql.SparkSession\ndf: org.apache.spark.sql.DataFrame = [cities: array<string>, name: string ... 1 more field]\nreport: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [cities: array<string>, name: string ... 1 more field]\n+----------+------+------------------+\n|    cities|  name|           schools|\n+----------+------+------------------+\n|[portland]|Justin|[[berkeley, 2014]]|\n+----------+------+------------------+\n\nrdd1: org.apache.spark.rdd.RDD[String] = /user/guest/flume/18-07-08/Log.1531015974737 MapPartitionsRDD[383] at textFile at <console>:208\nThis is the second three files test on Hadoop!!!!\nThis is the second three files test on Hadoop!!!!\nnewRDD: Long = 2\n2\nLines with a: 2\n"}]},"apps":[],"jobName":"paragraph_1531081667230_-838070840","id":"20180708-202747_621273847","dateCreated":"2018-07-08T20:27:47+0000","dateStarted":"2018-07-08T21:46:41+0000","dateFinished":"2018-07-08T21:46:58+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:235"},{"user":"anonymous","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala"},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1531082405470_-554420600","id":"20180708-204005_1205442141","dateCreated":"2018-07-08T20:40:05+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:236","text":"import org.apache.spark.sql.types._\r\n\r\nimport org.apache.spark.sql.SparkSession\r\n\r\nval rdd1 = spark.sparkContext.textFile(\"/user/guest/flume/18-07-08/Log.1531080665465\")\r\n\r\nval rdd2 = spark.sparkContext.textFile(\"/user/guest/flume/18-07-08/Log.1531015974737\")\r\n\r\nval rdd3 = spark.sparkContext.textFile(\"/user/guest/flume/18-07-08/Log.1531015921443\")\r\n\r\nrdd1.collect().foreach(println)\r\n\r\nrdd2.collect().foreach(println)\r\n\r\nrdd3.collect().foreach(println)\r\n\r\nval filteredRDD = rdd1.filter(line => line.contains(\"a\")).count()\r\n\r\nprintln(s\"Line with a: $filteredRDD\")","dateUpdated":"2018-07-08T22:01:42+0000","dateFinished":"2018-07-08T22:02:09+0000","dateStarted":"2018-07-08T22:01:45+0000","results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"import org.apache.spark.sql.types._\nimport org.apache.spark.sql.SparkSession\nrdd1: org.apache.spark.rdd.RDD[String] = /user/guest/flume/18-07-08/Log.1531080665465 MapPartitionsRDD[399] at textFile at <console>:218\nrdd2: org.apache.spark.rdd.RDD[String] = /user/guest/flume/18-07-08/Log.1531015974737 MapPartitionsRDD[401] at textFile at <console>:218\nrdd3: org.apache.spark.rdd.RDD[String] = /user/guest/flume/18-07-08/Log.1531015921443 MapPartitionsRDD[403] at textFile at <console>:218\n{\"name\":\"Michael\", \"cities\":[\"palo alto\", \"menlo park\"], \"schools\":[{\"sname\":\"stanford\", \"year\":2010}, {\"sname\":\"berkeley\", \"year\":2012}]}\n{\"name\":\"Andy\", \"cities\":[\"santa cruz\"], \"schools\":[{\"sname\":\"ucsb\", \"year\":2011}]}\n{\"name\":\"Justin\", \"cities\":[\"portland\"], \"schools\":[{\"sname\":\"berkeley\", \"year\":2014}]}\nThis is the second three files test on Hadoop!!!!\nThis is the second three files test on Hadoop!!!!\n{\n  \"squadName\": \"Super hero squad\",\n  \"homeTown\": \"Metro City\",\n  \"formed\": 2016,\n  \"secretBase\": \"Super tower\",\n  \"active\": true\n}\nCopy the CSV File to the Ambari !! FLUME\n`\nThis is some data to upload to hadoop. First test on Hadoop and Ambari....Success!!!!\nfilteredRDD: Long = 3\nLine with a: 3\n"}]}},{"user":"anonymous","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1531086925311_-2061092432","id":"20180708-215525_25591635","dateCreated":"2018-07-08T21:55:25+0000","status":"READY","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:514"}],"name":"LabNote","id":"2DMDN5AX9","angularObjects":{"2CHS8UYQQ:shared_process":[],"2C8A4SZ9T_livy2:shared_process":[],"2CK8A9MEG:shared_process":[],"2C4U48MY3_spark2:shared_process":[],"2CKAY1A8Y:shared_process":[],"2CKEKWY8Z:shared_process":[]},"config":{"isZeppelinNotebookCronEnable":false,"looknfeel":"default","personalizedMode":"false"},"info":{}}